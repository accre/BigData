#!/bin/bash
# batch-job.slurm

#SBATCH --ntasks=4
#SBATCH --cpus-per-task=2
#SBATCH --time=00:13:00
#SBATCH --mem-per-cpu=8G
#SBATCH --partition=debug
    

export JOB_ID=$SLURM_JOB_ID
export JOB_HOME="/scratch/$USER/$JOB_ID"
echo "JOB_HOME=$JOB_HOME"
mkdir -p $JOB_HOME 

export NWORKERS=$(( $SLURM_NTASKS - 2 ))
echo "NWORKERS=$NWORKERS"

# id of the last process
export LAST_PROC=$(( $SLURM_NTASKS - 1 ))


export SPARK_WORKER_DIR=$JOB_HOME/work
export SPARK_LOCAL_DIR=$JOB_HOME/tmp
mkdir -p $SPARK_WORKER_DIR $SPARK_LOCAL_DIR 

setpkgs -a spark_2.1.0 # sets JAVA_HOME and SPARK_HOME

export SPARK_MASTER_PORT=7077
export SPARK_MASTER_WEBUI_PORT=8080
    
# Create the configuration file specific to this batch job
echo "# This file has been generated by $0
0         ./task-roles.sh CLIENT
1         ./task-roles.sh MASTER
2-$LAST_PROC      ./task-roles.sh WORKER" \
> cluster.conf
    

export SPARK_DAEMON_MEMORY=1g
export SPARK_WORKER_CORES=$SLURM_CPUS_PER_TASK
export SPARK_WORKER_MEMORY=$(( SLURM_MEM_PER_CPU * $SLURM_CPUS_PER_TASK - 4000))m
export SPARK_EXECUTOR_CORES=$SLURM_CPUS_PER_TASK
export SPARK_EXECUTOR_MEMORY=$SPARK_WORKER_MEMORY

# Try to load stuff that the spark scripts will load
source "$SPARK_HOME/sbin/spark-config.sh"
source "$SPARK_HOME/bin/load-spark-env.sh"


# Specify input, output, and jar/java/Python files
# INPUT="README.md"
# INPUT="/scratch/arnoldjr/stack-archives/xml/ai.stackexchange.com/"
INPUT="/scratch/arnoldjr/stack-archives/xml/math.stackexchange.com/"
OUTPUT="wordcount_$JOB_ID"
export APP="spark-wc_2.11-1.0.jar $INPUT $OUTPUT"
echo $APP

# Start the CLIENT, MATER, and WORKERS
srun -l -o slurm-%j-task-%t.out --multi-prog cluster.conf
